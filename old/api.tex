 

\begin{frame}{Fonctions standards de string.h}
\begin{itemize} 
\item memset, memmove, memcpy, memcmp, memchr, etc...
\item strlen, strcpy, strcmp, strcat, strncat, strchr, etc...
\item  \c{void  *kmemdup(const void  *src,  size_t  len, gfp_t  gfp)},
\item \c{char *kstrdup(const char   *s,   gfp_t   gfp)}   et   \c{char
    *kstrndup(const char *s, size_t max, gfp_t gfp)}
\item sprintf, sscanf
\end{itemize} 
\end{frame} 

\begin{frame}{Les listes}
Implémentation assez singulière de listes génériques en C. 
\begin{lstlisting} 
#define container_of(ptr, type, member) ({ \ 
   const typeof( ((type *)0)->member ) *__mptr = (ptr); \
   (type *)( (char *)__mptr - offsetof(type,member) );})
\end{lstlisting} 
(Inclure linux/list.h)
\begin{itemize}
\item Declarer un type pour les noeuds de sa liste
\item Ajouter un attribut de type \c{struct list_head} à son type

\item Une liste vide est alors composé d'une simple \c{struct list_head} à déclarer ainsi: \c{LIST_HEAD(my_var)}
\item Pour les fonction d'ajout, suppression, etc... tout est géré avec les \c{list_head}
\item Pour accèder au données liées à un \c{list_head}, il faut utiliser la macro \c{} (qui utilise \c{container_of})
\end{itemize} 
\begin{lstlisting}
#include <linux/list.h>
stuct my_node_t {
  struct list_head node;
  /* other attributes */
}
void f() {
   static LIST_HEAD(my_list);
   struct my_node_t new_node; 
   struct my_node_t *i; 
   printf("%d\n", list_empty(my_list));
   list_add(&new_node->node, &my_list);
   printf("%d\n", list_size(my_list));
   list_for_each_entry(i, &my_list, node) {
   }
} 
\end{lstlisting} 
\end{frame} 

\begin{frame}{Barrière mémoire}
  Le CPU peut optimiser du code en réordonnancant les instructions. Ca
  peut être problèmatique dans le cas  des accès IO ou sur des système
  SMP. Utilisation de barrières (Documentation/memory-barriers.txt):
\begin{itemize}
\item rmb() : Pas de réordonnancement des lectures avant et après
\item wmb() : Pas de réordonnancement des écriture avant et après
\item mb() : rmb+wmb
\end{itemize} 
\end{frame} 


\section{Interruptions}

\subsection{wait queue}

\begin{frame}{API wait queue}
  \begin{itemize} 
Declarer:
  \item \c{DECLARE_WAIT_QUEUE_HEAD(my_queue)} : initialisation lors de la déclaration
  \item \c{wait_queue_head_t my_queue; init_waitqueue_head(&my_queue)}: Déclaration et initialisation séparés
Attendre
  \item \c{void wait_event(queue, condition)}: Attend un evenement sur la queue ET que la condition soit vérifiée
  \item \c{int wait_event_killable(queue, condition)}: Idem mais retourne une erreur si executé dans un appel system et le processus est tué avec SIGKILL
  \item \c{int wait_event_interruptible(queue, condition)}: Idem mais retourne une erreur si executé dans un appel system et le processus recoit un signal
  \item \c{void wait_event_timeout(queue, condition)}: Idem wait_event mais avec un timeout
  \item \c{void wait_event_tinterruptible_imeout(queue, condition)}: wait_event_timeout + wait_event_interruptible
Réveiller
\item \c{wait_up(queue)} Réveil tous les processus en attente sur la queue
\item \c{wait+uo_interruptible(queue)} Idem mais, seulement les interruptibles
  \end{itemize} 
\end{frame} 


\subsection{Gestion de la concurrence}

\begin{frame}{Désactivation des interruption et de la préemption}
local_irq_save(flags) : Sauve l'état des interuptions dans flags et désactive les interruptions 
local_irq_disable() : Désactive les interruptions
local_bh_disable();: Désactive les softirq (bottom halves)
preempt_disable(): Désactive la préemption

local_irq_disabled() : Retourne vrai si les interruption du CPU local sont désactivées

local_irq_restore(flags) : Sauve l'état des interuptions dans flags et désactive les interruptions 
local_irq_enable() : Désactive les interruptions
local_bh_enable();: Désactive les softirq (bottom halves)
preempt_enable(): Désactive la préemption
\end{frame} 

\begin{frame}{Spin Lock}
Dans certains cas,  une tâche peut avoir besoin
  d'un accès  exclusif à  une ressource potentiellement  utilisée dans
  une interruption. Dans  ca cas, il est necessaire  de désactiver les
  interruption   lors   de   l'accès   à  la   ressource.   Néanmoins,
  l'interruption  peut se  produire sur  un  autre CPU.  Il est  alors
  nécessaire de se protèger contre cette éventualité
  \c{linux/spinlock.h} 
\begin{itemize} 
\item Il s'agit d'une attente active sur une section critique
\item Ne traite pas les problème de concurence mais de parallèlisme uniquement. Par conséquent, uniquement en environnement SMP
\item Principalement utilisé lors de en complément de la désactivation des interruption
\item La préhemption est aussi désactivé durant un spin_lock
\end{itemize} 
\begin{itemize}
\item \c{DEFINE_SPINLOCK(lock)},  \c{spin_lock_init(spinlock_t *lock)} Initialise un spin lock
\item \c{spin_lock(spinlock_t *lock)}, \c{spin_trylock(spinlock_t *lock)}, \c{spin_unlock(spinlock_t *lock)} Idem que les fonction \c{mutex_*}
\item \c{spin_lock_irqsave(spinlock_t *lock)}, \c{spin_unlock_irqrestore(spinlock_t *lock)} Spinlock et désactive/réactive les interruptions sur le processeur courant. Les flags contiennent l'états du mask d'interruption
\item \c{spin_lock_bh(spinlock_t *lock}, \c{spin_unlock_bh(spinlock_t *lock)} spinlock et désactive/réactive les softirq.
\end{itemize} 
\end{frame} 

Faire un tableau
\begin{frame}{Alternatives}
  Opérations atomiques
  \begin{itemize}
  \item linux/atomic.h Documentation/atomic_ops.txt
  \item static atomic_t my_counter = ATOMIC_INIT(i) Initialisation
  \item void atomic_set(atomic_t *v, int i)
  \item int atomic_read(atomic_t *v)
        void atomic_inc(atomic_t *v)
        void atomic_dec(atomic_t *v)
        void atomic_add(int i, atomic_t *v)
        void atomic_sub(int i, atomic_t *v)
        int atomic_inc_return(atomic_t *v)
        int atomic_dec_return(atomic_t *v)
        int atomic_add_return(int i, atomic_t *v)
        int atomic_sub_return(int i, atomic_t *v)
        int atomic_inc_and_test(atomic_t *v)
        int atomic_dec_and_test(atomic_t *v)
        int atomic_xchg(atomic_t *v, int new)
        int atomic_cmpxchg(atomic_t *v, int old, int new)


        void set_bit(unsigned long nr, volatile unsigned long *addr)
        void clear_bit(unsigned long nr, volatile unsigned long *addr)
        void change_bit(unsigned long nr, volatile unsigned long *addr)
        int test_bit(unsigned long nr, volatile unsigned long *addr)
        int test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
        int test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
        int test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
        long cas(long *mem, long old, long new) (Compare and Swap)

  \item 
  \end{itemize} 
local_t (Documentation/local_ops.txt) Similaire aux atomic_t mais locaux à un CPU

 Read-Copy-Update (RCU)
\end{frame} 


\begin{frame}{ Debugguer}
Il est possible d'utiliser directemenr printk en specifiant le degré d'importance de l'information:
printk(KERN_ALERT)
Néanmoins, il est maintenant conseillé d'utiliser les macro pr_cont , pr_debug, pr_info, pr_notce, pr_warning, pr_err, pr_crit, pr_alert, pr_emerg

Les pr_debug ne sont compilé que si l'une des option CONFIG_DEBUG ou CONFIG_DYNAMIC_DEBUG est active. Dans le cas de DYNAMIC_DEBUG, il est posisble d'activer les messages namiquement en passant par debugfs (cf Documentation/dynamic-debug-howto.txt)
(mount -t debugfs none /sys/kernel/debug si nécessaire)

BUG()
BUG_ON(condition)
\end{frame} 
